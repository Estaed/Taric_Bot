"""
Metrics extraction module for Taric Bot AI.

This module combines all metrics calculation modules to analyze performance and behavior
from state-action pairs generated by the frame analysis system.

You can run this file directly:
    python src/metrics_extraction/metrics_extraction.py --input data/state_action_pairs --output data/metrics_data --format per_second
"""

import os
import sys
import json
import glob
import time
from pathlib import Path
import logging
import argparse
from tqdm import tqdm
from datetime import datetime

# Add parent directory to path when running directly
if __name__ == "__main__":
    # Add the project root to the path so we can import modules properly
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "../.."))
    sys.path.insert(0, project_root)

# Import modules - will work both when imported as a module and when run directly
from src.metrics_extraction.combat_metrics import calculate_combat_metrics
from src.metrics_extraction.vision_metrics import calculate_vision_metrics
from src.metrics_extraction.positioning_metrics import calculate_positioning_metrics
from src.metrics_extraction.mechanics_metrics import calculate_mechanics_metrics
from src.metrics_extraction.game_state_metrics import calculate_game_state_metrics
from src.metrics_extraction.file_organizer import organize_metric_files, organize_state_action_files
from src.config import RAW_DATA_DIR, STATE_ACTION_DIR, METRICS_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("metrics_extraction.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def get_match_id_from_filename(filename):
    """Extract match ID from filename."""
    # Example: taric_sa_pairs_OC1_666436490_20250515_195729.json
    parts = os.path.basename(filename).replace('.json', '').split('_')
    # Return the match ID part
    return '_'.join(parts[2:4])  # OC1_666436490


def extract_metrics_from_file(file_path, validate_zeros=True, output_format="summary"):
    """
    Extract all metrics from a single state-action pair file and combine with raw match data.
    
    Args:
        file_path (str): Path to the state-action pair JSON file
        validate_zeros (bool): Whether to validate metrics for suspicious zero values
        output_format (str): Format of the output - "summary" or "per_second"
        
    Returns:
        dict: Dictionary of all extracted metrics
    """
    logger.info(f"Processing file: {file_path}")
    
    try:
        # Load state-action pair data
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extract metadata and match data
        metadata = data.get('metadata', {})
        match_data = data.get('match_data', {})
        state_action_pairs = data.get('state_action_pairs', [])
        
        # Get match ID from filename for raw data lookup
        match_id = None
        filename = os.path.basename(file_path)
        parts = filename.split('_')
        if len(parts) >= 4:
            # Extract OC1_666495933 part
            match_id = f"{parts[2]}_{parts[3].split('.')[0]}"
        
        # Look for raw match data to enhance the metrics
        raw_match_data = {}
        if match_id:
            # Try to find the raw match file
            raw_match_file = find_raw_match_file(match_id)
            if raw_match_file:
                try:
                    with open(raw_match_file, 'r', encoding='utf-8') as f:
                        raw_match_data = json.load(f)
                    logger.info(f"Found raw match data: {raw_match_file}")
                except Exception as e:
                    logger.warning(f"Error loading raw match data: {e}")
        
        # Add start time for processing
        start_time = time.time()
        
        # Calculate all metrics
        logger.info("Calculating combat metrics...")
        combat_metrics = calculate_combat_metrics(state_action_pairs, match_data)
        
        logger.info("Calculating vision metrics...")
        vision_metrics = calculate_vision_metrics(state_action_pairs, match_data)
        
        # Enhance vision metrics with raw data if available
        if raw_match_data:
            vision_metrics = enhance_vision_metrics(vision_metrics, raw_match_data)
        
        logger.info("Calculating positioning metrics...")
        positioning_metrics = calculate_positioning_metrics(state_action_pairs, match_data)
        
        logger.info("Calculating mechanics metrics...")
        try:
            mechanics_metrics = calculate_mechanics_metrics(state_action_pairs, match_data)
            # Enhance mechanics metrics with raw data if available
            if raw_match_data:
                mechanics_metrics = enhance_mechanics_metrics(mechanics_metrics, raw_match_data)
        except Exception as e:
            logger.error(f"Error calculating mechanics metrics: {e}")
            mechanics_metrics = {}
        
        logger.info("Calculating game state metrics...")
        game_state_metrics = calculate_game_state_metrics(state_action_pairs, match_data)
        
        # Add end time and calculate duration
        end_time = time.time()
        processing_duration = end_time - start_time
        
        # Add processing metadata
        processing_metadata = {
            "processing_start_time": datetime.fromtimestamp(start_time).isoformat(),
            "processing_end_time": datetime.fromtimestamp(end_time).isoformat(),
            "processing_duration_seconds": processing_duration,
            "metrics_modules": [
                "combat_metrics",
                "vision_metrics",
                "positioning_metrics",
                "mechanics_metrics",
                "game_state_metrics"
            ]
        }
        
        # Combine all metrics
        all_metrics = {
            'match_id': match_id or 'unknown',
            'metadata': {
                **metadata,
                'processing': processing_metadata
            },
            'features': {
                'combat': combat_metrics,
                'vision': vision_metrics,
                'positioning': positioning_metrics,
                'mechanics': mechanics_metrics,
                'game_state': game_state_metrics
            }
        }
        
        # If per_second format is requested, add time-series data
        if output_format == "per_second":
            all_metrics['time_series'] = extract_time_series_data(state_action_pairs)
        
        # Validate for zero values if requested
        if validate_zeros:
            validation_results = validate_metrics_for_zeros(all_metrics['features'])
            all_metrics['metadata']['validation'] = validation_results
            
            if validation_results['has_suspicious_zeros']:
                logger.warning(f"Found suspicious zero values in {match_id}")
                for category, metrics in validation_results['suspicious_zero_metrics'].items():
                    if metrics:
                        logger.warning(f"  {category}: {', '.join(metrics)}")
        
        return all_metrics
        
    except Exception as e:
        logger.error(f"Error processing {file_path}: {str(e)}")
        return None

def find_raw_match_file(match_id):
    """
    Find the raw match file for a given match ID.
    
    Args:
        match_id (str): Match ID to look for
        
    Returns:
        str or None: Path to the raw match file if found, None otherwise
    """
    # Look in the raw data directory
    pattern = os.path.join(RAW_DATA_DIR, f"*{match_id}*.json")
    matches = glob.glob(pattern)
    if matches:
        return matches[0]
    
    # Try alternate raw data directories if needed
    alt_pattern = os.path.join(RAW_DATA_DIR, "**", f"*{match_id}*.json")
    matches = glob.glob(alt_pattern, recursive=True)
    if matches:
        return matches[0]
    
    # If still not found, look in the entire data directory
    last_chance_pattern = os.path.join(os.path.dirname(RAW_DATA_DIR), "**", f"*{match_id}*.json")
    matches = glob.glob(last_chance_pattern, recursive=True)
    if matches:
        return matches[0]
    
    return None

def enhance_vision_metrics(vision_metrics, raw_match_data):
    """
    Enhance vision metrics with data from the raw match file.
    
    Args:
        vision_metrics (dict): Vision metrics calculated from state-action pairs
        raw_match_data (dict): Raw match data
        
    Returns:
        dict: Enhanced vision metrics
    """
    # Make a copy to avoid modifying the original
    enhanced_metrics = vision_metrics.copy()
    
    try:
        # Find the player (Taric) in the match data
        participant_data = None
        participants = raw_match_data.get('participants', [])
        for participant in participants:
            # Look for Taric in participants
            if participant.get('championName', '').lower() == 'taric' or participant.get('championId') == 44:
                participant_data = participant
                break
        
        if participant_data:
            # Extract relevant ward data
            stats = participant_data.get('stats', {})
            timeline = participant_data.get('timeline', {})
            
            # If ward fields show 0 but raw data has wards, update them
            if enhanced_metrics.get('total_wards_placed', 0) == 0:
                enhanced_metrics['total_wards_placed'] = stats.get('wardsPlaced', 0)
            
            if enhanced_metrics.get('wards_cleared', 0) == 0:
                enhanced_metrics['wards_cleared'] = stats.get('wardsKilled', 0)
            
            if enhanced_metrics.get('vision_wards_purchased', 0) == 0:
                enhanced_metrics['vision_wards_purchased'] = stats.get('sightWardsBoughtInGame', 0)
            
            if enhanced_metrics.get('control_wards_purchased', 0) == 0:
                enhanced_metrics['control_wards_purchased'] = stats.get('visionWardsBoughtInGame', 0)
            
            # If ward type data is available, update ward counts by type
            if enhanced_metrics.get('wards_by_type', {}).get('stealth_ward', 0) == 0:
                wards_by_type = enhanced_metrics.get('wards_by_type', {})
                wards_by_type['stealth_ward'] = stats.get('sightWardsBoughtInGame', 0) 
                wards_by_type['control_ward'] = stats.get('visionWardsBoughtInGame', 0)
                enhanced_metrics['wards_by_type'] = wards_by_type
            
            # Find ward placement events in timeline
            ward_events = []
            frames = raw_match_data.get('timeline', {}).get('frames', [])
            participant_id = participant_data.get('participantId', -1)
            
            for frame in frames:
                events = frame.get('events', [])
                for event in events:
                    if event.get('type') == 'WARD_PLACED' and event.get('creatorId') == participant_id:
                        ward_events.append(event)
            
            # If we found timeline ward events, update ward placements by region
            if ward_events and 'wards_by_region' in enhanced_metrics:
                wards_by_region = enhanced_metrics.get('wards_by_region', {})
                for event in ward_events:
                    # Map position to a region (simplified example)
                    position = event.get('position', {})
                    x, y = position.get('x', 0), position.get('y', 0)
                    region = get_map_region(x, y)
                    if region not in wards_by_region:
                        wards_by_region[region] = 0
                    wards_by_region[region] += 1
                enhanced_metrics['wards_by_region'] = wards_by_region
            
            # Calculate ward coverage based on ward placements
            if ward_events and 'ward_coverage_by_phase' in enhanced_metrics:
                early_wards = 0
                mid_wards = 0
                late_wards = 0
                
                for event in ward_events:
                    timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds
                    if timestamp < 900:  # 15 minutes
                        early_wards += 1
                    elif timestamp < 1800:  # 30 minutes
                        mid_wards += 1
                    else:
                        late_wards += 1
                
                coverage_by_phase = enhanced_metrics.get('ward_coverage_by_phase', {})
                if 'early_game' in coverage_by_phase and coverage_by_phase['early_game'] == 0:
                    coverage_by_phase['early_game'] = normalize_ward_coverage(early_wards, 5)
                if 'mid_game' in coverage_by_phase and coverage_by_phase['mid_game'] == 0:
                    coverage_by_phase['mid_game'] = normalize_ward_coverage(mid_wards, 7)
                if 'late_game' in coverage_by_phase and coverage_by_phase['late_game'] == 0:
                    coverage_by_phase['late_game'] = normalize_ward_coverage(late_wards, 5)
                
                enhanced_metrics['ward_coverage_by_phase'] = coverage_by_phase
            
            # If no overall ward coverage but we have ward placements, calculate it
            if enhanced_metrics.get('overall_ward_coverage', 0) == 0 and ward_events:
                enhanced_metrics['overall_ward_coverage'] = len(ward_events) / 15  # Arbitrary scale
    
    except Exception as e:
        logger.warning(f"Error enhancing vision metrics: {e}")
    
    return enhanced_metrics

def enhance_mechanics_metrics(mechanics_metrics, raw_match_data):
    """
    Enhance mechanics metrics with data from the raw match file.
    
    Args:
        mechanics_metrics (dict): Mechanics metrics calculated from state-action pairs
        raw_match_data (dict): Raw match data
        
    Returns:
        dict: Enhanced mechanics metrics
    """
    # Make a copy to avoid modifying the original
    enhanced_metrics = mechanics_metrics.copy()
    
    try:
        # Find the player (Taric) in the match data
        participant_data = None
        participants = raw_match_data.get('participants', [])
        for participant in participants:
            # Look for Taric in participants
            if participant.get('championName', '').lower() == 'taric' or participant.get('championId') == 44:
                participant_data = participant
                break
        
        if participant_data:
            # Add any raw mechanics metrics if available
            stats = participant_data.get('stats', {})
            
            # Add CS per minute if not present
            if enhanced_metrics.get('cs_per_minute', 0) == 0:
                game_duration_minutes = raw_match_data.get('gameDuration', 0) / 60
                if game_duration_minutes > 0:
                    cs = stats.get('totalMinionsKilled', 0) + stats.get('neutralMinionsKilled', 0)
                    enhanced_metrics['cs_per_minute'] = cs / game_duration_minutes
            
            # Add KDA if not present
            if enhanced_metrics.get('kda', 0) == 0:
                kills = stats.get('kills', 0)
                deaths = stats.get('deaths', 0)
                assists = stats.get('assists', 0)
                
                # Avoid division by zero
                if deaths > 0:
                    enhanced_metrics['kda'] = (kills + assists) / deaths
                else:
                    enhanced_metrics['kda'] = kills + assists
            
            # Enhance click metrics with APM if available
            if enhanced_metrics.get('clicks_per_minute', 0) == 0:
                apm = participant_data.get('timeline', {}).get('actions_per_minute')
                if apm:
                    enhanced_metrics['clicks_per_minute'] = apm
            
            # Enhance ability usage if available
            if raw_match_data.get('timeline') and enhanced_metrics.get('ability_usage', {}).get('q_accuracy', 0) == 0:
                # Simple estimation based on kill participation
                kill_participation = stats.get('killParticipation', 0)
                if kill_participation > 0:
                    ability_usage = enhanced_metrics.get('ability_usage', {})
                    ability_usage['q_accuracy'] = 0.65 * kill_participation  # Estimate
                    ability_usage['w_accuracy'] = 0.70 * kill_participation  # Estimate
                    ability_usage['e_accuracy'] = 0.60 * kill_participation  # Estimate
                    ability_usage['r_accuracy'] = 0.80 * kill_participation  # Estimate
                    enhanced_metrics['ability_usage'] = ability_usage
    
    except Exception as e:
        logger.warning(f"Error enhancing mechanics metrics: {e}")
    
    return enhanced_metrics

def get_map_region(x, y):
    """
    Get the map region for a given position.
    
    Args:
        x (int): X coordinate
        y (int): Y coordinate
        
    Returns:
        str: Map region
    """
    # Simplified region mapping based on quadrants
    if x < 5000 and y < 5000:
        return 'bottom_blue'
    elif x < 5000 and y >= 5000:
        return 'top_blue'
    elif x >= 5000 and y < 5000:
        return 'bottom_red'
    else:
        return 'top_red'

def normalize_ward_coverage(ward_count, expected_wards):
    """
    Normalize ward coverage score.
    
    Args:
        ward_count (int): Number of wards placed
        expected_wards (int): Expected number of wards
        
    Returns:
        float: Normalized ward coverage score (0-1)
    """
    if expected_wards <= 0:
        return 0
    
    return min(1.0, ward_count / expected_wards)

def get_item_name(item_id):
    """
    Get the name of an item from its ID.
    
    Args:
        item_id (int): Item ID
        
    Returns:
        str: Item name
    """
    # Simplified mapping for common items
    item_map = {
        2055: 'Control Ward',
        3340: 'Warding Totem',
        3364: 'Oracle Lens',
        3401: 'Remnant of the Aspect',
        3107: 'Redemption',
        3190: 'Locket of the Iron Solari'
    }
    
    return item_map.get(item_id, f"Item {item_id}")

def is_active_item(item_id):
    """
    Check if an item is an active item.
    
    Args:
        item_id (int): Item ID
        
    Returns:
        bool: True if the item is an active item, False otherwise
    """
    # List of active items commonly used by Taric
    active_items = [
        2055,  # Control Ward
        3340,  # Warding Totem
        3364,  # Oracle Lens
        3504,  # Ardent Censer
        3107,  # Redemption
        3190,  # Locket of the Iron Solari
        3222,  # Mikael's Crucible
        3401,  # Remnant of the Aspect
        2065,  # Shurelya's Battlesong
        3050,  # Zeke's Convergence
        3109,  # Knight's Vow
        3193   # Gargoyle Stoneplate
    ]
    
    return item_id in active_items

def extract_time_series_data(state_action_pairs):
    """
    Extract time-series data from state-action pairs for per-second metrics.
    
    Args:
        state_action_pairs (list): List of state-action pairs
        
    Returns:
        dict: Dictionary of time-series data
    """
    time_series = {
        'timestamp': [],
        'combat': {
            'health_percent': [],
            'mana_percent': [],
            'damage_dealt': [],
            'healing_done': [],
            'shield_amount': []
        },
        'positioning': {
            'x_position': [],
            'y_position': [],
            'distance_to_ally_adc': [],
            'enemies_nearby': [],
            'map_region': []
        },
        'mechanics': {
            'abilities_used': [],
            'q_available': [],
            'w_available': [],
            'e_available': [],
            'r_available': []
        },
        'vision': {
            'wards_placed': [],
            'vision_score': []
        },
        'game_state': {
            'gold': [],
            'level': [],
            'in_teamfight': [],
            'in_combat': []
        }
    }
    
    for pair in state_action_pairs:
        if not isinstance(pair, dict) or 'state' not in pair:
            continue
            
        state = pair['state']
        action = pair.get('action', {})
        
        # Get timestamp
        timestamp = state.get('game_time_seconds', 0)
        time_series['timestamp'].append(timestamp)
        
        # Combat metrics
        taric_state = state.get('taric_state', {})
        time_series['combat']['health_percent'].append(taric_state.get('health_percent', 0))
        time_series['combat']['mana_percent'].append(taric_state.get('mana_percent', 0))
        time_series['combat']['damage_dealt'].append(taric_state.get('damage_dealt_last_frame', 0))
        time_series['combat']['healing_done'].append(taric_state.get('healing_done_last_frame', 0))
        time_series['combat']['shield_amount'].append(taric_state.get('shield_amount', 0))
        
        # Positioning metrics
        x_pos = taric_state.get('position_x', 0)
        y_pos = taric_state.get('position_y', 0)
        time_series['positioning']['x_position'].append(x_pos)
        time_series['positioning']['y_position'].append(y_pos)
        time_series['positioning']['map_region'].append(get_map_region(x_pos, y_pos))
        
        # Find ADC in nearby allies
        adc_distance = 9999
        nearby_allies = state.get('nearby_units', {}).get('allies', [])
        for ally in nearby_allies:
            if ally.get('role', '').lower() == 'adc' or ally.get('is_adc', False):
                ally_x = ally.get('position_x', 0)
                ally_y = ally.get('position_y', 0)
                distance = ((ally_x - x_pos) ** 2 + (ally_y - y_pos) ** 2) ** 0.5
                adc_distance = min(adc_distance, distance)
        
        time_series['positioning']['distance_to_ally_adc'].append(adc_distance)
        time_series['positioning']['enemies_nearby'].append(len(state.get('nearby_units', {}).get('enemies', [])))
        
        # Mechanics metrics
        cooldowns = taric_state.get('cooldowns', {})
        time_series['mechanics']['q_available'].append(cooldowns.get('Q', 0) <= 0)
        time_series['mechanics']['w_available'].append(cooldowns.get('W', 0) <= 0)
        time_series['mechanics']['e_available'].append(cooldowns.get('E', 0) <= 0)
        time_series['mechanics']['r_available'].append(cooldowns.get('R', 0) <= 0)
        
        # Track which ability was used
        ability_used = action.get('ability', '')
        time_series['mechanics']['abilities_used'].append(ability_used if ability_used else '')
        
        # Vision metrics
        vision_state = state.get('vision_state', {})
        time_series['vision']['wards_placed'].append(vision_state.get('wards_placed_last_frame', 0))
        time_series['vision']['vision_score'].append(vision_state.get('vision_score', 0))
        
        # Game state metrics
        time_series['game_state']['gold'].append(taric_state.get('gold', 0))
        time_series['game_state']['level'].append(taric_state.get('level', 1))
        time_series['game_state']['in_teamfight'].append(state.get('in_teamfight', False))
        time_series['game_state']['in_combat'].append(taric_state.get('in_combat', False))
    
    return time_series

def validate_metrics_for_zeros(features):
    """
    Validate metrics for suspicious zero values.
    
    Args:
        features (dict): Features dictionary with combat, vision, etc. categories
        
    Returns:
        dict: Validation results
    """
    validation_results = {
        'has_suspicious_zeros': False,
        'suspicious_zero_metrics': {
            'combat': [],
            'vision': [],
            'positioning': [],
            'mechanics': [],
            'game_state': []
        }
    }
    
    # Combat metrics that should never be zero
    combat_essential_metrics = [
        'total_healing', 'healing_per_minute', 'w_link_uptime', 'w_link_efficiency',
        'shield_efficiency', 'damage_prevented_per_minute', 'prevention_efficiency'
    ]
    
    # Vision metrics that should never be zero
    vision_essential_metrics = [
        'vision_score_per_minute', 'ward_placement_efficiency'
    ]
    
    # Positioning metrics that should never be zero
    positioning_essential_metrics = [
        'position_change_rate', 'lane_positioning_score', 'teamfight_positioning_score'
    ]
    
    # Mechanics metrics that should never be zero
    mechanics_essential_metrics = [
        'ability_accuracy', 'reaction_time_score', 'combo_execution_score'
    ]
    
    # Game state metrics that should never be zero
    game_state_essential_metrics = [
        'teamfight_participation_rate', 'objective_participation_rate'
    ]
    
    # Check combat metrics
    combat_metrics = features.get('combat', {})
    for metric in combat_essential_metrics:
        if metric in combat_metrics and combat_metrics[metric] == 0:
            validation_results['has_suspicious_zeros'] = True
            validation_results['suspicious_zero_metrics']['combat'].append(metric)
    
    # Check vision metrics
    vision_metrics = features.get('vision', {})
    for metric in vision_essential_metrics:
        if metric in vision_metrics and vision_metrics[metric] == 0:
            validation_results['has_suspicious_zeros'] = True
            validation_results['suspicious_zero_metrics']['vision'].append(metric)
    
    # Check positioning metrics
    positioning_metrics = features.get('positioning', {})
    for metric in positioning_essential_metrics:
        if metric in positioning_metrics and positioning_metrics[metric] == 0:
            validation_results['has_suspicious_zeros'] = True
            validation_results['suspicious_zero_metrics']['positioning'].append(metric)
    
    # Check mechanics metrics
    mechanics_metrics = features.get('mechanics', {})
    for metric in mechanics_essential_metrics:
        if metric in mechanics_metrics and mechanics_metrics[metric] == 0:
            validation_results['has_suspicious_zeros'] = True
            validation_results['suspicious_zero_metrics']['mechanics'].append(metric)
    
    # Check game state metrics
    game_state_metrics = features.get('game_state', {})
    for metric in game_state_essential_metrics:
        if metric in game_state_metrics and game_state_metrics[metric] == 0:
            validation_results['has_suspicious_zeros'] = True
            validation_results['suspicious_zero_metrics']['game_state'].append(metric)
    
    # Special case - check teamfight_events abilities_used
    if 'game_state' in features:
        teamfight_events = features['game_state'].get('teamfight_events', [])
        for i, event in enumerate(teamfight_events):
            if not event.get('abilities_used'):
                validation_results['has_suspicious_zeros'] = True
                validation_results['suspicious_zero_metrics']['game_state'].append(f"teamfight_event_{i}_abilities_used")
    
    return validation_results

def extract_metrics_to_file(file_path, output_dir, validate_zeros=True, output_format="summary"):
    """
    Extract all metrics from a file and save to output directory.
    
    Args:
        file_path (str): Path to the state-action pair JSON file
        output_dir (str): Directory to save the extracted metrics
        validate_zeros (bool): Whether to validate metrics for suspicious zero values
        output_format (str): Format of the output - "summary" or "per_second"
    
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract metrics
        metrics = extract_metrics_from_file(file_path, validate_zeros, output_format)
        if not metrics:
            return False
        
        # Create output filename
        match_id = metrics.get('match_id', 'unknown')
        format_suffix = "per_second" if output_format == "per_second" else "summary"
        output_file = os.path.join(output_dir, f"taric_metrics_{match_id}_{format_suffix}.json")
        
        # Save metrics to file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info(f"Successfully saved metrics to {output_file}")
        return True
    
    except Exception as e:
        logger.error(f"Error saving metrics from {file_path}: {str(e)}")
        return False

def process_all_files(input_dir, output_dir, batch_size=5, organize_files=True, validate_zeros=True, output_format="summary"):
    """
    Process all state-action pair files in the input directory.
    
    Args:
        input_dir (str): Directory containing state-action pair files
        output_dir (str): Directory to save the extracted metrics
        batch_size (int): Number of files to process in each batch
        organize_files (bool): Whether to organize output files
        validate_zeros (bool): Whether to validate metrics for suspicious zero values
        output_format (str): Format of the output - "summary" or "per_second"
    
    Returns:
        dict: Processing results with counts and summary
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Find all state-action pair files
    input_path = Path(input_dir)
    sa_files = list(input_path.glob("taric_sa_pairs_*.json"))
    
    if not sa_files:
        logger.warning(f"No state-action pair files found in {input_dir}")
        return {
            "total_files": 0,
            "successful_files": 0,
            "failed_files": 0,
            "summary": None
        }
    
    logger.info(f"Found {len(sa_files)} state-action pair files to process")
    
    # Process files in batches
    total_processed = 0
    total_success = 0
    
    # Create subdirectory for this format if needed
    format_dir = os.path.join(output_dir, output_format)
    os.makedirs(format_dir, exist_ok=True)
    
    for i in range(0, len(sa_files), batch_size):
        batch_files = sa_files[i:i+batch_size]
        logger.info(f"Processing batch {i//batch_size + 1}/{(len(sa_files)-1)//batch_size + 1}")
        
        for file_path in tqdm(batch_files, desc=f"Batch {i//batch_size + 1}"):
            total_processed += 1
            success = extract_metrics_to_file(file_path, format_dir, validate_zeros, output_format)
            if success:
                total_success += 1
    
    # Organize output files if requested
    if organize_files and total_success > 0:
        logger.info("Organizing extracted metric files...")
        organize_metric_files(format_dir)
    
    success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0
    logger.info(f"Processed {total_processed} files with {total_success} successes ({success_rate:.1f}%)")
    
    # Generate summary report
    summary = generate_summary_report(format_dir)
    
    return {
        "total_files": total_processed,
        "successful_files": total_success,
        "failed_files": total_processed - total_success,
        "summary": summary
    }

def generate_summary_report(output_dir):
    """
    Generate a summary report of the extracted metrics.
    
    Args:
        output_dir (str): Directory containing metric files
        
    Returns:
        dict: Summary report data
    """
    # Find all metric files
    metrics_path = Path(output_dir)
    metrics_files = list(metrics_path.glob("**/*taric_metrics_*.json"))
    
    if not metrics_files:
        logger.warning(f"No metric files found in {output_dir}")
        return None
    
    # Initialize summary data
    summary = {
        "total_files": len(metrics_files),
        "metrics_present": {
            "combat": 0,
            "vision": 0,
            "positioning": 0,
            "mechanics": 0,
            "game_state": 0
        },
        "game_modes": {},
        "champions": {},
        "total_state_actions": 0,
        "validation_results": {
            "files_with_suspicious_zeros": 0,
            "suspicious_metrics": {
                "combat": set(),
                "vision": set(),
                "positioning": set(),
                "mechanics": set(),
                "game_state": set()
            }
        },
        "generated_at": datetime.now().isoformat()
    }
    
    # Process each file for summary
    for file_path in tqdm(metrics_files, desc="Generating summary"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Count metrics present
            for metric_type in summary["metrics_present"]:
                if metric_type in data.get('features', {}):
                    summary["metrics_present"][metric_type] += 1
            
            # Count game modes
            game_mode = data.get('metadata', {}).get('game_mode', 'unknown')
            summary["game_modes"][game_mode] = summary["game_modes"].get(game_mode, 0) + 1
            
            # Count champions (allies and enemies)
            allies = data.get('metadata', {}).get('allies', [])
            enemies = data.get('metadata', {}).get('enemies', [])
            
            for champ in allies + enemies:
                champion_name = champ.get('champion', 'unknown')
                summary["champions"][champion_name] = summary["champions"].get(champion_name, 0) + 1
            
            # Count total state-action pairs
            sa_count = data.get('metadata', {}).get('state_action_count', 0)
            summary["total_state_actions"] += sa_count
            
            # Check validation results
            validation = data.get('metadata', {}).get('validation', {})
            if validation.get('has_suspicious_zeros', False):
                summary["validation_results"]["files_with_suspicious_zeros"] += 1
                
                for metric_type, metrics in validation.get('suspicious_zero_metrics', {}).items():
                    for metric in metrics:
                        summary["validation_results"]["suspicious_metrics"][metric_type].add(metric)
            
        except Exception as e:
            logger.error(f"Error processing {file_path} for summary: {str(e)}")
    
    # Convert sets to lists for JSON serialization
    for metric_type in summary["validation_results"]["suspicious_metrics"]:
        summary["validation_results"]["suspicious_metrics"][metric_type] = list(
            summary["validation_results"]["suspicious_metrics"][metric_type]
        )
    
    # Save summary report
    summary_path = os.path.join(output_dir, "extraction_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2)
    
    logger.info(f"Summary report generated at {summary_path}")
    
    # Print validation summary
    if summary["validation_results"]["files_with_suspicious_zeros"] > 0:
        logger.warning(f"Found {summary['validation_results']['files_with_suspicious_zeros']} files with suspicious zero values")
        for metric_type, metrics in summary["validation_results"]["suspicious_metrics"].items():
            if metrics:
                logger.warning(f"  {metric_type}: {', '.join(metrics)}")
    else:
        logger.info("No suspicious zero values found in the processed files!")
    
    return summary

def main():
    """Main function when running the module directly."""
    parser = argparse.ArgumentParser(description='Run metrics extraction pipeline')
    parser.add_argument('--input', default=STATE_ACTION_DIR, help='Directory containing state-action pair files')
    parser.add_argument('--output', default=METRICS_DIR, help='Directory to save extracted metrics files')
    parser.add_argument('--batch-size', type=int, default=5, help='Number of files to process in each batch')
    parser.add_argument('--no-organize', action='store_true', help='Skip organizing output files')
    parser.add_argument('--no-validation', action='store_true', help='Skip validating metrics for zero values')
    parser.add_argument('--summary-only', action='store_true', help='Only generate summary without reprocessing files')
    parser.add_argument('--format', choices=['summary', 'per_second'], default='summary', 
                        help='Output format: summary (aggregated stats) or per_second (time series)')
    
    args = parser.parse_args()
    
    if args.summary_only:
        logger.info("Generating summary report only...")
        format_dir = os.path.join(args.output, args.format)
        os.makedirs(format_dir, exist_ok=True)
        generate_summary_report(format_dir)
    else:
        logger.info(f"Processing state-action pair files from {args.input}")
        logger.info(f"Output will be saved to {args.output} in {args.format} format")
        
        result = process_all_files(
            args.input, 
            args.output,
            batch_size=args.batch_size,
            organize_files=not args.no_organize,
            validate_zeros=not args.no_validation,
            output_format=args.format
        )
        
        logger.info(f"Processed {result['total_files']} files")
        logger.info(f"  Successful: {result['successful_files']}")
        logger.info(f"  Failed: {result['failed_files']}")
        
    logger.info("Metrics extraction completed successfully")

if __name__ == "__main__":
    main() 